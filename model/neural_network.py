# -*- coding: utf-8 -*-
"""Neural Network.ipynb

Automatically generated by Colaboratory.

### Install Packages
"""

import sys
!{sys.executable} -m pip install dfply

"""### Import Library"""

import pandas as pd
import numpy as np
import time
from dfply import *
import matplotlib.pyplot as plt
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn

import tensorflow as tf

"""### Import Award Data"""

!git clone https://github.com/floraxhuang/Oscar.git

# %cd Oscar
awards = pd.read_csv('award_tag.csv')

#preprocessing
awards = awards[["Rater","Film","Win"]]

awards.head()

"""### Import Oscar data"""

oscars = pd.read_csv("oscar_test_tag.csv")

#preprocessing
oscars = oscars[["Rater","Film","Year","Win"]]

oscars.tail()

"""### Tensorflow

#### Training
"""

tf.keras.backend.clear_session()

num_movies = awards.Film.nunique()
num_raters = awards.Rater.nunique()

# Convert DataFrame in user-item matrix
matrix = awards.pivot(index='Rater', columns='Film', values='Win')
matrix.fillna(0, inplace=True)

# Users and items ordered as they are in matrix
raters = matrix.index.tolist()
movies = matrix.columns.tolist()
matrix = matrix.as_matrix()

# Network Parameters
num_input = num_movies
num_hidden_1 = 10
num_hidden_2 = 5
X = tf.placeholder(tf.float64, [None, num_input])
weights = {
    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1], dtype=tf.float64)),
    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2], dtype=tf.float64)),
    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1], dtype=tf.float64)),
    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input], dtype=tf.float64)),
}
biases = {
    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),
    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2], dtype=tf.float64)),
    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),
    'decoder_b2': tf.Variable(tf.random_normal([num_input], dtype=tf.float64)),
}

# Building the encoder
def encoder(x):
    # Encoder Hidden layer with sigmoid activation #1
    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']), biases['encoder_b1']))
    # Encoder Hidden layer with sigmoid activation #2
    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))
    return layer_2
# Building the decoder
def decoder(x):
    # Decoder Hidden layer with sigmoid activation #1
    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']), biases['decoder_b1']))
    # Decoder Hidden layer with sigmoid activation #2
    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']), biases['decoder_b2']))
    return layer_2

# Construct model
encoder_op = encoder(X)
decoder_op = decoder(encoder_op)
# Prediction
y_pred = decoder_op
# Targets are the input data.
y_true = X

# Define loss and optimizer, minimize the squared error
loss = tf.losses.mean_squared_error(y_true, y_pred)
optimizer = tf.train.RMSPropOptimizer(0.03).minimize(loss)
predictions = pd.DataFrame()
# Define evaluation metrics
eval_x = tf.placeholder(tf.int32, )
eval_y = tf.placeholder(tf.int32, )
pre, pre_op = tf.metrics.precision(labels=eval_x, predictions=eval_y)

# Initialize the variables (i.e. assign their default value)
init = tf.global_variables_initializer()
local_init = tf.local_variables_initializer()

with tf.Session() as session:
    epochs = 100

    session.run(init)
    session.run(local_init)

    num_batches = 3
    matrix = np.array_split(matrix, num_batches)
    
    loss_list=[]
    for i in range(epochs):
      avg_cost = 0
      for batch in matrix:
        _, l = session.run([optimizer, loss], feed_dict={X: batch})
        avg_cost += l
      
      avg_cost /= num_batches
      loss_list.append(avg_cost)

    print("Predictions...")
    matrix = np.concatenate(matrix, axis=0)
    preds = session.run(decoder_op, feed_dict={X: matrix})
    predictions = predictions.append(pd.DataFrame(preds))

    predictions = predictions.stack().reset_index(name='Predictions')
    predictions.columns = ['Rater', 'Film', 'Predictions']
    predictions['Rater'] = predictions['Rater'].map(lambda value: raters[value])
    predictions['Film'] = predictions['Film'].map(lambda value: movies[value])

"""### Oscar Predictions"""

plt.plot(range(epochs), loss_list)
plt.title("Loss over Epoch")

print("Filter out items in training set")
keys = ['Rater', 'Film']
i1 = predictions.set_index(keys).index
i2 = awards.set_index(keys).index

recs = predictions[~i1.isin(i2)]
recs = recs.sort_values(['Rater', 'Predictions'], ascending=[True, False])

pred_res = pd.merge(oscars,recs,how="left",on=["Rater","Film"])

appended_data = []
for y in np.unique(pred_res.Year):
  subset = pred_res[pred_res["Year"]==y]
  max_prob = subset["Predictions"].max()
  for n in subset.index:
    if subset.loc[n,"Predictions"] == max_prob:
      subset.loc[n,"Predictions"] = 1
    else:
      subset.loc[n,"Predictions"] = -1
  appended_data.append(subset)

oscar_pred = pd.concat(appended_data, axis=0)

oscar_pred[oscar_pred["Win"]==1]

oscar_pred[oscar_pred["Predictions"]==1]

